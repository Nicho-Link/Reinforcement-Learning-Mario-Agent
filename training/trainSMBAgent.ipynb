{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation, TransformObservation, Monitor\n",
    "import numpy as np\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Import own Functions\n",
    "from helper_functions.create_Plot import plot_results\n",
    "from helper_functions.create_Agent import MarioAgentEpsilonGreedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\n",
    "    ['NOOP'],\n",
    "    ['A'],\n",
    "    ['B'],\n",
    "    ['A', 'B'],    \n",
    "    ['right'],\n",
    "    ['left'],\n",
    "    ['right', 'A'],\n",
    "    ['left', 'A'],\n",
    "    ['right', 'B'],\n",
    "    ['left', 'B'],\n",
    "    ['right', 'A', 'B'],\n",
    "    ['left', 'A', 'B'],\n",
    "    ['up'],\n",
    "    ['down']\n",
    "]\n",
    "buffer_size = 25000\n",
    "batch_size = 32\n",
    "learning_rate = 0.00025\n",
    "\n",
    "stacking_number = 10\n",
    "# skipping_number = 4    # Not implemented\n",
    "\n",
    "online_update_every = 3\n",
    "exp_before_target_sync = 10000\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99\n",
    "num_episodes = 1000\n",
    "\n",
    "plot_every = 25\n",
    "save_every = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vid_folder = os.path.join(\"videos\")\n",
    "exp_before_training = batch_size + 5\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, action_space)\n",
    "env = Monitor(env, vid_folder, video_callable=lambda episode_id: True, force=True)\n",
    "# env = SkipFrame(env, skip=skipping_number) # Not implemented\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, lambda obs: np.squeeze(obs, axis=-1))\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=stacking_number)\n",
    "\n",
    "state = env.reset()\n",
    "state_shape = state.shape\n",
    "model_folder = os.path.join(\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "checkpoint_folder = os.path.join(\"checkpoints\")\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "starting_point = None # os.path.join(model_folder, \"model_50.pth\")\n",
    "plot_folder = os.path.join(\"logs\")\n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder)\n",
    "\n",
    "mario = MarioAgentEpsilonGreedy(num_actions=len(action_space), state_shape=state_shape, checkpoint_folder=checkpoint_folder, model_folder=model_folder, wantcuda=True, starting_point=starting_point, learning_rate=learning_rate, epsilon_start=epsilon_start, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, batch_size=32, gamma=gamma, buffer_size=buffer_size, exp_before_training=exp_before_training, online_update_every=online_update_every, exp_before_target_sync=exp_before_target_sync, save_every=save_every)\n",
    "\n",
    "reward_list = []\n",
    "steps_list = []\n",
    "q_list = []\n",
    "loss_list = []\n",
    "epsilon_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\OneDrive\\Dokumente\\A - DHBW\\6. Semester\\Reinforcement Learning\\Abgaben\\Coding\\Reinforcement-Learning\\helper_functions\\create_Agent.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  state = torch.tensor(state, dtype=torch.float32, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 abgeschlossen mit 2748 Schritten, Gesamtbelohnung: -15, Epsilon: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\OneDrive\\Dokumente\\A - DHBW\\6. Semester\\Reinforcement Learning\\Abgaben\\Coding\\Reinforcement-Learning\\venv\\Lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 abgeschlossen mit 2356 Schritten, Gesamtbelohnung: -15, Epsilon: 0.995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, num_episodes+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        #env.render() # Visualize\n",
    "        action = mario.selectAction(state)\n",
    "        steps = steps + 1\n",
    "        next_state, reward, resetnow, info = env.step(action)\n",
    "        mario.saveExp(state, action, next_state, reward, resetnow)\n",
    "        q, loss = mario.learn_get_TDest_loss()\n",
    "        state = next_state\n",
    "        if resetnow or info['flag_get']:\n",
    "            break\n",
    "    print(f\"Episode {episode} abgeschlossen mit {steps} Schritten, Gesamtbelohnung: {reward}, Epsilon: {mario.epsilon}\\n\\n\")\n",
    "    \n",
    "    reward_list.append(reward)\n",
    "    steps_list.append(steps)\n",
    "    q_list.append(q)\n",
    "    loss_list.append(loss)\n",
    "    epsilon_list.append(mario.epsilon)\n",
    "\n",
    "    if episode % plot_every == 0:\n",
    "        plot_results(reward_list, steps_list, q_list, loss_list, epsilon_list, os.path.join(plot_folder, f\"plot_{episode}.png\"))\n",
    "    \n",
    "    if episode % save_every == 0:\n",
    "        torch.save(dict(model=mario.model.state_dict(), optimizer=mario.optimizer.state_dict(), epsilon=mario.epsilon), os.path.join(checkpoint_folder, f\"model_ep{episode}.pth\"))\n",
    "    \n",
    "    mario.decayEpsilon()\n",
    "\n",
    "torch.save(dict(model=mario.model.state_dict(), optimizer=mario.optimizer.state_dict(), epsilon=mario.epsilon), os.path.join(model_folder, f\"final_model.pth\"))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
