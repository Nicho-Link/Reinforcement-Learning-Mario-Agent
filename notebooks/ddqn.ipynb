{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Deep Q Newtork (DDQN) for Super Mario Bros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import gym_super_mario_bros\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.wrappers import (\n",
    "    FrameStack,\n",
    "    GrayScaleObservation,\n",
    "    Monitor,\n",
    "    ResizeObservation,\n",
    "    TransformObservation,\n",
    ")\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from tqdm import tqdm\n",
    "\n",
    "from setup import create_directories\n",
    "from src.DDQNAgent import MarioAgentEpsilonGreedy\n",
    "from src.utils import plot_results\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\n",
    "#     \"mps\"\n",
    "#     if torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "np.random.seed(42)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_version = None\n",
    "agent = \"ddqn\"\n",
    "create_directories(agent)\n",
    "\n",
    "model_folder = (\n",
    "    os.path.join(\"models\", agent)\n",
    "    if train_version == None\n",
    "    else os.path.join(\"models\", f\"{agent}_v{train_version}\")\n",
    ")\n",
    "checkpoint_folder = os.path.join(model_folder, \"checkpoints\")\n",
    "\n",
    "videos_folder = (\n",
    "    os.path.join(\"references\", agent, \"videos\")\n",
    "    if train_version == None\n",
    "    else os.path.join(\"references\", f\"{agent}_v{train_version}\", \"videos\")\n",
    ")\n",
    "plot_folder = (\n",
    "    os.path.join(\"references\", agent, \"images\")\n",
    "    if train_version == None\n",
    "    else os.path.join(\"references\", f\"{agent}_v{train_version}\", \"images\")\n",
    ")\n",
    "evaluation_folder = (\n",
    "    os.path.join(\"references\", agent, \"evaluation\")\n",
    "    if train_version == None\n",
    "    else os.path.join(\"references\", f\"{agent}_v{train_version}\", \"evaluation\")\n",
    ")\n",
    "\n",
    "\n",
    "# set path for checkpoint to load from\n",
    "starting_point = None  # os.path.join(checkpoint_folder, \"model_ep850.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\n",
    "    [\"NOOP\"],\n",
    "    [\"A\"],\n",
    "    [\"B\"],\n",
    "    [\"right\"],\n",
    "    [\"left\"],\n",
    "    [\"right\", \"A\"],\n",
    "    [\"right\", \"B\"],\n",
    "    [\"right\", \"A\", \"B\"],\n",
    "]\n",
    "buffer_size = 25000\n",
    "batch_size = 64\n",
    "learning_rate = 0.00009\n",
    "stacking_number = 10\n",
    "online_update_every = 3\n",
    "exp_before_target_sync = 5000\n",
    "exp_before_training = batch_size + 5\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.001\n",
    "gamma = 0.99\n",
    "num_episodes = 1000\n",
    "plot_every = 25\n",
    "save_every = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment and other variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "env = JoypadSpace(env, action_space)\n",
    "env = Monitor(env, videos_folder, video_callable=lambda episode_id: True, force=True)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, lambda obs: np.squeeze(obs, axis=-1))\n",
    "env = TransformObservation(env, f=lambda x: x / 255.0)\n",
    "env = FrameStack(env, num_stack=stacking_number)\n",
    "\n",
    "state = env.reset()\n",
    "state_shape = state.shape\n",
    "\n",
    "\n",
    "mario = MarioAgentEpsilonGreedy(\n",
    "    num_actions=len(action_space),\n",
    "    state_shape=state_shape,\n",
    "    checkpoint_folder=checkpoint_folder,\n",
    "    model_folder=model_folder,\n",
    "    wantcuda=True,\n",
    "    starting_point=starting_point,\n",
    "    learning_rate=learning_rate,\n",
    "    epsilon_start=epsilon_start,\n",
    "    epsilon_min=epsilon_min,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma,\n",
    "    buffer_size=buffer_size,\n",
    "    exp_before_training=exp_before_training,\n",
    "    online_update_every=online_update_every,\n",
    "    exp_before_target_sync=exp_before_target_sync,\n",
    "    save_every=save_every,\n",
    ")\n",
    "\n",
    "reward_list = []\n",
    "steps_list = []\n",
    "q_list = []\n",
    "loss_list = []\n",
    "epsilon_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(1, num_episodes + 1), total=num_episodes, desc=\"Training\"):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    mean_episode_q = []\n",
    "    mean_episode_loss = []\n",
    "    # Initiate loop for the current episode to play the game until it ends\n",
    "    while True:\n",
    "        # To visualize the training, uncomment the following line\n",
    "        # env.render()\n",
    "        action = mario.selectAction(state)\n",
    "        next_state, reward, resetnow, info = env.step(action)\n",
    "        mario.saveExp(state, action, next_state, reward, resetnow)\n",
    "        q, loss = mario.learn_get_TDest_loss()\n",
    "        state = next_state\n",
    "        total_reward = total_reward + reward\n",
    "        steps = steps + 1\n",
    "        mean_episode_q.append(q)\n",
    "        mean_episode_loss.append(loss)\n",
    "        if resetnow or info[\"flag_get\"]:\n",
    "            break\n",
    "    print(\n",
    "        f\"Episode {episode} abgeschlossen mit {steps} Schritten, Gesamtbelohnung: {total_reward}, Epsilon: {mario.epsilon}\\n\\n\"\n",
    "    )\n",
    "    # Save the results of the current episode\n",
    "    reward_list.append(total_reward)\n",
    "    steps_list.append(steps)\n",
    "    q_list.append(np.mean(mean_episode_q))\n",
    "    loss_list.append(np.mean(mean_episode_loss))\n",
    "    epsilon_list.append(mario.epsilon)\n",
    "\n",
    "    # Plot the results of all episodes at the defined intervals\n",
    "    if episode % 1 == 0:\n",
    "        plot_results(\n",
    "            reward_list=reward_list,\n",
    "            steps_list=steps_list,\n",
    "            q_list=q_list,\n",
    "            loss_list=loss_list,\n",
    "            epsilon_list=epsilon_list,\n",
    "            save_fig=True,\n",
    "            save_path=os.path.join(plot_folder, f\"plot_{episode}.png\"),\n",
    "        )\n",
    "\n",
    "    # Save the model at the defined intervals\n",
    "    if episode % 1 == 0:\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=mario.online.state_dict(),\n",
    "                optimizer=mario.optimizer.state_dict(),\n",
    "                epsilon=mario.epsilon,\n",
    "            ),\n",
    "            os.path.join(checkpoint_folder, f\"model_ep{episode}.pth\"),\n",
    "        )\n",
    "\n",
    "    # Decay the epsilon value at the end of each episode\n",
    "    mario.decayEpsilon(strat=\"lin\")\n",
    "\n",
    "# Save the final model after all episodes are completed\n",
    "torch.save(\n",
    "    dict(\n",
    "        model=mario.online.state_dict(),\n",
    "        optimizer=mario.optimizer.state_dict(),\n",
    "        epsilon=mario.epsilon,\n",
    "    ),\n",
    "    os.path.join(model_folder, f\"final_model.pth\"),\n",
    ")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "starting_point = os.path.join(model_folder, f\"{agent}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment and other variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_number = 10\n",
    "num_episodes = 10\n",
    "\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "env = JoypadSpace(env, action_space)\n",
    "env = Monitor(env, videos_folder, video_callable=lambda episode_id: True, force=True)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, lambda obs: np.squeeze(obs, axis=-1))\n",
    "env = TransformObservation(env, f=lambda x: x / 255.0)\n",
    "env = FrameStack(env, num_stack=stacking_number)\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "state_shape = state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = mario = MarioAgentEpsilonGreedy(\n",
    "    num_actions=len(action_space),\n",
    "    state_shape=state_shape,\n",
    "    wantcuda=True,\n",
    "    checkpoint_folder=None,\n",
    "    model_folder=None,\n",
    "    starting_point=starting_point,\n",
    ")\n",
    "agent.epsilon = 0.0\n",
    "agent.model.eval()\n",
    "\n",
    "reward_list = []\n",
    "steps_list = []\n",
    "q_list = []\n",
    "loss_list = []\n",
    "epsilon_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(1, num_episodes + 1)):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    mean_episode_q = []\n",
    "    mean_episode_loss = []\n",
    "    resetnow = False\n",
    "    # Initiate loop for the current episode to play the game until it ends\n",
    "    while resetnow == False:\n",
    "        # To visualize the game\n",
    "        env.render()\n",
    "        action = agent.selectAction(state)\n",
    "        next_state, reward, resetnow, info = env.step(action)\n",
    "        resetnow = resetnow\n",
    "        agent.saveExp(state, action, next_state, reward, resetnow)\n",
    "        q, loss = agent.learn_get_TDest_loss()\n",
    "        state = next_state\n",
    "        total_reward = total_reward + reward\n",
    "        steps = steps + 1\n",
    "        mean_episode_q.append(q)\n",
    "        mean_episode_loss.append(loss)\n",
    "    print(\n",
    "        f\"Episode {episode} abgeschlossen mit {steps} Schritten, Gesamtbelohnung: {total_reward}, Epsilon: {agent.epsilon}\\n\\n\"\n",
    "    )\n",
    "    # Save the results of the current episode\n",
    "    reward_list.append(total_reward)\n",
    "    steps_list.append(steps)\n",
    "    q_list.append(np.mean(mean_episode_q))\n",
    "    loss_list.append(np.mean(mean_episode_loss))\n",
    "    epsilon_list.append(agent.epsilon)\n",
    "\n",
    "# Plot the results of all episodes at the defined intervals\n",
    "plot_results(\n",
    "    reward_list,\n",
    "    steps_list,\n",
    "    q_list,\n",
    "    loss_list,\n",
    "    epsilon_list,\n",
    "    os.path.join(plot_folder, f\"plot_{episode}.png\"),\n",
    ")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
