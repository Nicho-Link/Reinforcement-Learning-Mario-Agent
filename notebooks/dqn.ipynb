{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network (DQN) for Super Mario Bros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT, SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from setup import create_directories\n",
    "from src.DQN import DQN\n",
    "from src.ExperienceBuffer import ExperienceReplayBuffer\n",
    "from src.utils import plot_results, preprocess_frame, preprocess_state\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"SuperMarioBros-1-1-v0\"\n",
    "env = gym_super_mario_bros.make(ENV_NAME)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observation Space: \", env.observation_space.shape)\n",
    "print(\"Action Space: \", env.action_space.n)\n",
    "print(SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "axes = axes.flatten()\n",
    "for i in range(4):\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    state = preprocess_frame(state)\n",
    "\n",
    "    axes[i].imshow(state)\n",
    "    axes[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_version = None\n",
    "agent = \"dqn\"\n",
    "create_directories(agent)\n",
    "\n",
    "model_folder = (\n",
    "    os.path.join(\"models\", agent)\n",
    "    if train_version == None\n",
    "    else os.path.join(\"models\", f\"{agent}_v{train_version}\")\n",
    ")\n",
    "checkpoint_folder = os.path.join(model_folder, \"checkpoints\")\n",
    "\n",
    "videos_folder = (\n",
    "    os.path.join(\"references\", agent, \"videos\")\n",
    "    if train_version == None\n",
    "    else os.path.join(\"references\", f\"{agent}_v{train_version}\", \"videos\")\n",
    ")\n",
    "plot_folder = (\n",
    "    os.path.join(\"references\", agent, \"images\")\n",
    "    if train_version == None\n",
    "    else os.path.join(\"references\", f\"{agent}_v{train_version}\", \"images\")\n",
    ")\n",
    "evaluation_folder = (\n",
    "    os.path.join(\"references\", agent, \"evaluation\")\n",
    "    if train_version == None\n",
    "    else os.path.join(\"references\", f\"{agent}_v{train_version}\", \"evaluation\")\n",
    ")\n",
    "\n",
    "\n",
    "# set path for checkpoint to load from\n",
    "starting_point = None  # os.path.join(checkpoint_folder, \"model_ep850.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"SuperMarioBros-1-1-v0\"\n",
    "GAMMA = 0.99\n",
    "N_EPISODES = 1000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "# uncomment the following line to use exponential decay\n",
    "# EPSILON_DECAY = 0.995\n",
    "# uncomment the following line to use linear decay\n",
    "EPSILON_DECAY = (EPSILON_START - EPSILON_END) / N_EPISODES\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "REPLAY_BUFFER = 10000\n",
    "BATCH_SIZE = 64\n",
    "TARGET_UPDATE = 10\n",
    "CHECKPOINT_RATE = 100\n",
    "N_STACKD_FRAMES = 4\n",
    "INPUT_SHAPE = (4, 84, 84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "dqn = DQN(INPUT_SHAPE, env.action_space.n).to(device)\n",
    "target_net = DQN(INPUT_SHAPE, env.action_space.n).to(device)\n",
    "target_net.load_state_dict(dqn.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = Adam(dqn.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = ExperienceReplayBuffer(REPLAY_BUFFER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = EPSILON_START\n",
    "episode_rewards = list()\n",
    "episode_durations = list()\n",
    "episode_steps = list()\n",
    "q_values = list()\n",
    "losses = list()\n",
    "epsilons = list()\n",
    "\n",
    "for episode in tqdm(range(1, N_EPISODES + 1), total=N_EPISODES, desc=\"Training\"):\n",
    "    frame_stack = collections.deque(maxlen=N_STACKD_FRAMES)\n",
    "    start = time.time()\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(\n",
    "        state, frame_stack=frame_stack, stack_size=N_STACKD_FRAMES\n",
    "    ).to(device)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_steps = 0\n",
    "    while not done or info[\"flag_get\"]:\n",
    "\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            action = dqn(state).argmax().item()\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_state(\n",
    "            next_state, frame_stack=frame_stack, stack_size=N_STACKD_FRAMES\n",
    "        ).to(device)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer.buffer) > BATCH_SIZE:\n",
    "            exp = replay_buffer.sample(BATCH_SIZE)\n",
    "            (\n",
    "                sample_states,\n",
    "                sample_actions,\n",
    "                sample_rewards,\n",
    "                sample_next_states,\n",
    "                sample_dones,\n",
    "            ) = zip(*exp)\n",
    "\n",
    "            sample_states = torch.cat(sample_states).to(device)\n",
    "            sample_actions = torch.LongTensor(sample_actions).to(device)\n",
    "            sample_rewards = torch.FloatTensor(sample_rewards).to(device)\n",
    "            sample_next_states = torch.cat(sample_next_states).to(device)\n",
    "            sample_dones = torch.FloatTensor(sample_dones).to(device)\n",
    "\n",
    "            q_curr_state = (\n",
    "                dqn(sample_states).gather(1, sample_actions.unsqueeze(-1)).squeeze(-1)\n",
    "            )\n",
    "            q_next_state = target_net(sample_next_states).max(dim=1)[0]\n",
    "            expected_q = sample_rewards + GAMMA * q_next_state * (1 - sample_dones)\n",
    "\n",
    "            loss = nn.MSELoss()(q_curr_state, expected_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # clip gradients to stabilize training\n",
    "            nn.utils.clip_grad_norm_(dqn.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            q_values.append(q_curr_state.mean().item())\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(dqn.state_dict())\n",
    "\n",
    "    # update epsilion\n",
    "    # uncomment the following line to use exponential decay\n",
    "    # epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "    # uncomment the following line to use linear decay\n",
    "    epsilon = max(EPSILON_END, epsilon - EPSILON_DECAY)\n",
    "\n",
    "    # save metrics\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_durations.append(time.time() - start)\n",
    "    episode_steps.append(total_steps)\n",
    "    losses.append(loss.item())\n",
    "    epsilons.append(epsilon)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        plot_results(\n",
    "            reward_list=episode_rewards,\n",
    "            steps_list=episode_durations,\n",
    "            q_list=q_values,\n",
    "            loss_list=losses,\n",
    "            epsilon_list=epsilons,\n",
    "            save_fig=True,\n",
    "            save_path=os.path.join(plot_folder, f\"results_{episode}\"),\n",
    "        )\n",
    "\n",
    "    if episode % CHECKPOINT_RATE == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_folder, f\"checkpoint_{episode}.pth\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"episode\": episode,\n",
    "                \"model_state_dict\": dqn.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "                \"epsilon\": epsilon,\n",
    "            },\n",
    "            checkpoint_path,\n",
    "        )\n",
    "\n",
    "env.close()\n",
    "torch.save(dqn.state_dict(), os.path.join(model_folder, f\"{agent}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "eval_env = JoypadSpace(eval_env, SIMPLE_MOVEMENT)\n",
    "eval_env = gym.wrappers.Monitor(\n",
    "    eval_env, \"videos\", video_callable=lambda episode_id: True, force=True\n",
    ")\n",
    "\n",
    "# uncomment the following lines to load the model from saved checkpoint\n",
    "dqn = DQN(INPUT_SHAPE, eval_env.action_space.n).to(device)\n",
    "dqn.load_state_dict(torch.load(\"dqn.pth\"))\n",
    "dqn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_episode_rewards = list()\n",
    "eval_episode_durations = list()\n",
    "\n",
    "for episode in tqdm(range(1, 51), total=50, desc=\"Evaluation\"):\n",
    "    frame_stack = collections.deque(maxlen=N_STACKD_FRAMES)\n",
    "    start = time.time()\n",
    "    state = eval_env.reset()\n",
    "    state = preprocess_state(\n",
    "        state, frame_stack=frame_stack, stack_size=N_STACKD_FRAMES\n",
    "    ).to(device)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = dqn(state).argmax().item()\n",
    "        next_state, reward, done, info = eval_env.step(action)\n",
    "        done = done or info[\"flag_get\"]\n",
    "\n",
    "        next_state = preprocess_state(\n",
    "            next_state, frame_stack=frame_stack, stack_size=N_STACKD_FRAMES\n",
    "        ).to(device)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    eval_episode_rewards.append(total_reward)\n",
    "    eval_episode_durations.append(time.time() - start)\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(20, 10), nrows=2, ncols=1)\n",
    "axes = axes.flatten()\n",
    "axes[0].plot(\n",
    "    np.arange(len(eval_episode_rewards)),\n",
    "    eval_episode_rewards,\n",
    "    label=\"Episode Rewards\",\n",
    ")\n",
    "axes[0].axhline(np.mean(eval_episode_rewards), color=\"r\", linestyle=\"--\", label=\"Mean\")\n",
    "axes[0].set_xlabel(\"Episode\")\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "axes[0].set_title(\"Evaluation Episode Rewards\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(\n",
    "    np.arange(len(eval_episode_durations)), eval_episode_durations, label=\"Duration\"\n",
    ")\n",
    "axes[1].axhline(\n",
    "    np.mean(eval_episode_durations), color=\"r\", linestyle=\"--\", label=\"Mean\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Episode\")\n",
    "axes[1].set_ylabel(\"Duration\")\n",
    "axes[1].set_title(\"Evaluation Episode Durations\\n (in seconds)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
