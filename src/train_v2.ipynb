{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import (\n",
    "    FrameStack,\n",
    "    GrayScaleObservation,\n",
    "    ResizeObservation,\n",
    "    TransformObservation,\n",
    "    Monitor,\n",
    ")\n",
    "\n",
    "# Import own Functions\n",
    "from src.helper_functions.create_Plot import plot_results\n",
    "from src.helper_functions.create_Agent import MarioAgentEpsilonGreedy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [\n",
    "    [\"NOOP\"],\n",
    "    [\"A\"],\n",
    "    [\"B\"],\n",
    "    [\"right\"],\n",
    "    [\"left\"],\n",
    "    [\"right\", \"A\"],\n",
    "    [\"right\", \"B\"],\n",
    "    [\"right\", \"A\", \"B\"],\n",
    "]\n",
    "buffer_size = 25000\n",
    "batch_size = 64\n",
    "learning_rate = 0.00009\n",
    "stacking_number = 10\n",
    "online_update_every = 3\n",
    "exp_before_target_sync = 5000\n",
    "epsilon_start = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.001\n",
    "gamma = 0.99\n",
    "num_episodes = 1000\n",
    "plot_every = 25\n",
    "save_every = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this version-number to create new folders\n",
    "train_version = \"v2\"\n",
    "\n",
    "trainfolder = \"training_\" + train_version\n",
    "vid_folder = os.path.join(\"res\", trainfolder, \"all_videos\")\n",
    "\n",
    "model_folder = os.path.join(\"models\")\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "checkpoint_folder = os.path.join(model_folder, trainfolder, \"checkpoints\")\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "\n",
    "plot_folder = os.path.join(\"res\", trainfolder, \"plots\")\n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder)\n",
    "\n",
    "starting_point = None  # os.path.join(checkpoint_folder, \"model_ep850.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_before_training = batch_size + 5\n",
    "\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "env = JoypadSpace(env, action_space)\n",
    "env = Monitor(env, vid_folder, video_callable=lambda episode_id: True, force=True)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, lambda obs: np.squeeze(obs, axis=-1))\n",
    "env = TransformObservation(env, f=lambda x: x / 255.0)\n",
    "env = FrameStack(env, num_stack=stacking_number)\n",
    "\n",
    "state = env.reset()\n",
    "state_shape = state.shape\n",
    "\n",
    "\n",
    "mario = MarioAgentEpsilonGreedy(\n",
    "    num_actions=len(action_space),\n",
    "    state_shape=state_shape,\n",
    "    checkpoint_folder=checkpoint_folder,\n",
    "    model_folder=model_folder,\n",
    "    wantcuda=True,\n",
    "    starting_point=starting_point,\n",
    "    learning_rate=learning_rate,\n",
    "    epsilon_start=epsilon_start,\n",
    "    epsilon_min=epsilon_min,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma,\n",
    "    buffer_size=buffer_size,\n",
    "    exp_before_training=exp_before_training,\n",
    "    online_update_every=online_update_every,\n",
    "    exp_before_target_sync=exp_before_target_sync,\n",
    "    save_every=save_every,\n",
    ")\n",
    "\n",
    "reward_list = []\n",
    "steps_list = []\n",
    "q_list = []\n",
    "loss_list = []\n",
    "epsilon_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\OneDrive\\Dokumente\\A - DHBW\\6. Semester\\Reinforcement Learning\\Abgaben\\Coding\\Reinforcement-Learning\\src\\helper_functions\\create_Agent.py:104: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
      "c:\\Users\\nicho\\OneDrive\\Dokumente\\A - DHBW\\6. Semester\\Reinforcement Learning\\Abgaben\\Coding\\Reinforcement-Learning\\venv\\Lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 abgeschlossen mit 8019 Schritten, Gesamtbelohnung: 138.0, Epsilon: 1.0\n",
      "\n",
      "\n",
      "Episode 2 abgeschlossen mit 166 Schritten, Gesamtbelohnung: 235.0, Epsilon: 0.999\n",
      "\n",
      "\n",
      "Episode 3 abgeschlossen mit 8019 Schritten, Gesamtbelohnung: 136.0, Epsilon: 0.998\n",
      "\n",
      "\n",
      "Episode 4 abgeschlossen mit 214 Schritten, Gesamtbelohnung: 236.0, Epsilon: 0.997\n",
      "\n",
      "\n",
      "Episode 5 abgeschlossen mit 8019 Schritten, Gesamtbelohnung: 139.0, Epsilon: 0.996\n",
      "\n",
      "\n",
      "Episode 6 abgeschlossen mit 8019 Schritten, Gesamtbelohnung: 139.0, Epsilon: 0.995\n",
      "\n",
      "\n",
      "Episode 7 abgeschlossen mit 8019 Schritten, Gesamtbelohnung: 137.0, Epsilon: 0.994\n",
      "\n",
      "\n",
      "Episode 8 abgeschlossen mit 684 Schritten, Gesamtbelohnung: 633.0, Epsilon: 0.993\n",
      "\n",
      "\n",
      "Episode 9 abgeschlossen mit 8019 Schritten, Gesamtbelohnung: 139.0, Epsilon: 0.992\n",
      "\n",
      "\n",
      "Episode 10 abgeschlossen mit 180 Schritten, Gesamtbelohnung: 230.0, Epsilon: 0.991\n",
      "\n",
      "\n",
      "Episode 11 abgeschlossen mit 232 Schritten, Gesamtbelohnung: 239.0, Epsilon: 0.99\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, num_episodes + 1):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    mean_episode_q = []\n",
    "    mean_episode_loss = []\n",
    "    # Initiate loop for the current episode to play the game until it ends\n",
    "    while True:\n",
    "        # To visualize the training, uncomment the following line\n",
    "        # env.render()\n",
    "        action = mario.selectAction(state)\n",
    "        next_state, reward, resetnow, info = env.step(action)\n",
    "        mario.saveExp(state, action, next_state, reward, resetnow)\n",
    "        q, loss = mario.learn_get_TDest_loss()\n",
    "        state = next_state\n",
    "        total_reward = total_reward + reward\n",
    "        steps = steps + 1\n",
    "        mean_episode_q.append(q)\n",
    "        mean_episode_loss.append(loss)\n",
    "        if resetnow or info[\"flag_get\"]:\n",
    "            break\n",
    "    print(\n",
    "        f\"Episode {episode} abgeschlossen mit {steps} Schritten, Gesamtbelohnung: {total_reward}, Epsilon: {mario.epsilon}\\n\\n\"\n",
    "    )\n",
    "    # Save the results of the current episode\n",
    "    reward_list.append(total_reward)\n",
    "    steps_list.append(steps)\n",
    "    q_list.append(np.mean(mean_episode_q))\n",
    "    loss_list.append(np.mean(mean_episode_loss))\n",
    "    epsilon_list.append(mario.epsilon)\n",
    "\n",
    "    # Plot the results of all episodes at the defined intervals\n",
    "    if episode % plot_every == 0:\n",
    "        plot_results(\n",
    "            reward_list,\n",
    "            steps_list,\n",
    "            q_list,\n",
    "            loss_list,\n",
    "            epsilon_list,\n",
    "            os.path.join(plot_folder, f\"plot_{episode}.png\"),\n",
    "        )\n",
    "\n",
    "    # Save the model at the defined intervals\n",
    "    if episode % save_every == 0:\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=mario.model.state_dict(),\n",
    "                optimizer=mario.optimizer.state_dict(),\n",
    "                epsilon=mario.epsilon,\n",
    "            ),\n",
    "            os.path.join(checkpoint_folder, f\"model_ep{episode}.pth\"),\n",
    "        )\n",
    "\n",
    "    # Decay the epsilon value at the end of each episode\n",
    "    mario.decayEpsilon(strat=\"lin\")\n",
    "\n",
    "# Save the final model after all episodes are completed\n",
    "torch.save(\n",
    "    dict(\n",
    "        model=mario.model.state_dict(),\n",
    "        optimizer=mario.optimizer.state_dict(),\n",
    "        epsilon=mario.epsilon,\n",
    "    ),\n",
    "    os.path.join(model_folder, f\"final_model.pth\"),\n",
    ")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
